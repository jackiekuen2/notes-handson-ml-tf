{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch4_ExerciseQ12_EarlyStopping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/T4dm9OnQNLP87UPoDmiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackiekuen2/notes-handson-ml-tf/blob/master/ch4_ExerciseQ12_EarlyStopping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuSbN_t1gBjb",
        "colab_type": "text"
      },
      "source": [
        "# Q12. Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cIAbTuEixRy",
        "colab_type": "text"
      },
      "source": [
        "## I. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLbF8vmJfeLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43OmUwygsxs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e209719b-9177-46d1-96e7-c976641c349a"
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "list(iris.keys())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stxDimZXg_4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = iris.data[:, (2, 3)] #petal length, petal width\n",
        "y = iris.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PKnf0AGhGfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add the bias term for every instance (x0 = 1)\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Bz34iohnzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e6ee63a5-9998-425d-d4f5-b61e6c5f7a6d"
      },
      "source": [
        "X_with_bias[:5]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1. , 1.4, 0.2],\n",
              "       [1. , 1.4, 0.2],\n",
              "       [1. , 1.3, 0.2],\n",
              "       [1. , 1.5, 0.2],\n",
              "       [1. , 1.4, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh1-0wnshtHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-7Bf6PhjEtW",
        "colab_type": "text"
      },
      "source": [
        "## II. Train-test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytHjl2x6iGOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_size = len(X_with_bias)\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjWZA00WkAgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle the dataset\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tWvLrcblA5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert test data to one-hot encoding\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1 # (0, 1, 2) --> total 3 classes\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes)) # label all 0's first\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot\n",
        "\n",
        "\n",
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxtYiSyrggx",
        "colab_type": "text"
      },
      "source": [
        "## III. Training a Softmax model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NblJNEidqyeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps/exp_sums"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkstCEkvrakX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_inputs = X_train.shape[1] # 2 features + bias teem = 3 inputs\n",
        "n_outputs = len(np.unique(y_train)) # 3 classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw2x__oer3kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b676236d-809a-4181-fd07-206ea2d3fe21"
      },
      "source": [
        "# Train the softmax model\n",
        "eta = 0.01\n",
        "n_iterations = 10001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs) # Initialize with random figures\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta = Theta - eta * gradients"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.5356045081790177\n",
            "500 0.7698276617097016\n",
            "1000 0.6394784332731978\n",
            "1500 0.5618741363839648\n",
            "2000 0.5095831080853221\n",
            "2500 0.47127377559909306\n",
            "3000 0.44155863305230325\n",
            "3500 0.41755986648041216\n",
            "4000 0.3975941721521857\n",
            "4500 0.38060484552797946\n",
            "5000 0.3658905593000994\n",
            "5500 0.35296466414435634\n",
            "6000 0.34147705259255917\n",
            "6500 0.33116823861572764\n",
            "7000 0.3218410572511107\n",
            "7500 0.3133425076366726\n",
            "8000 0.30555169703144486\n",
            "8500 0.298371595843712\n",
            "9000 0.29172325270873556\n",
            "9500 0.2855416438026909\n",
            "10000 0.2797726355460243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78wdW_NCtBiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9e2bb527-4029-46b0-d711-17a0daadb899"
      },
      "source": [
        "Theta"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.62016071, -1.50976478, -4.92912459],\n",
              "       [-0.846549  ,  0.69453656,  0.25669351],\n",
              "       [-1.37970405, -0.04055292,  3.33700883]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvbi_3eTtIKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eb779b7-69bc-4824-e90e-3dd32d4401f7"
      },
      "source": [
        "# Predictions for validation set\n",
        "\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_pred = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean((y_pred == y_valid))\n",
        "accuracy_score"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20yvO3lrwUP5",
        "colab_type": "text"
      },
      "source": [
        "##IV. Softmax model + L2 Regularization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS-IK8dZwrSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6b04a332-2426-4763-b8df-9c0ccbf28272"
      },
      "source": [
        "eta = 0.01\n",
        "n_iterations = 10001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # regularization hyperparameter\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 4.074160805836161\n",
            "500 0.842423506343155\n",
            "1000 0.7077659962126894\n",
            "1500 0.6367414013918682\n",
            "2000 0.5952648892708439\n",
            "2500 0.5689591903221461\n",
            "3000 0.551156976597229\n",
            "3500 0.5384699412305943\n",
            "4000 0.529040469145154\n",
            "4500 0.5217862918782573\n",
            "5000 0.5160448760335734\n",
            "5500 0.5113933844416791\n",
            "6000 0.5075519836677346\n",
            "6500 0.5043293709329311\n",
            "7000 0.5015908506698222\n",
            "7500 0.4992389882701472\n",
            "8000 0.49720153694764324\n",
            "8500 0.49542370257763185\n",
            "9000 0.4938630670775326\n",
            "9500 0.4924861812387983\n",
            "10000 0.49126622945561005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "76a72012-4c67-4297-ee26-ee77e6910286",
        "id": "QPTdWIqpxGwo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Predictions for validation set\n",
        "\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_pred = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean((y_pred == y_valid))\n",
        "accuracy_score"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgasQldwxTW4",
        "colab_type": "text"
      },
      "source": [
        "## V. Softmax model + L2 Regularization + Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76c59bF9xNO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "ef599bba-4df0-47c8-ae2a-9c0f777c919e"
      },
      "source": [
        "eta = 0.01\n",
        "n_iterations = 10001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # regularization hyperparameter\n",
        "best_loss = np.infty\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    # Early stopping\n",
        "    logits = X_valid.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "    else:\n",
        "        print(iteration - 1, best_loss)\n",
        "        print(iteration, loss, \"Reaching the best loss, Early stopping!\")\n",
        "        break"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.1499523939452976\n",
            "500 1.1331943621656417\n",
            "1000 0.8449214146527371\n",
            "1500 0.7191016939830108\n",
            "2000 0.6595407268250023\n",
            "2500 0.626994603586099\n",
            "3000 0.6069434147592495\n",
            "3500 0.5934704952736416\n",
            "4000 0.5838241988882021\n",
            "4500 0.5765729480190931\n",
            "5000 0.570907156969735\n",
            "5500 0.5663401875471603\n",
            "6000 0.562565511240924\n",
            "6500 0.5593824514592203\n",
            "7000 0.5566550855491896\n",
            "7500 0.5542884213522633\n",
            "8000 0.5522140869292202\n",
            "8500 0.5503814821687892\n",
            "9000 0.5487521696314078\n",
            "9500 0.5472962380511188\n",
            "10000 0.5459898947672274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwWT7LglyTr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cc097bf-c3a6-4b64-9c48-792770e49159"
      },
      "source": [
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_pred = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_pred == y_valid)\n",
        "accuracy_score"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    }
  ]
}